"""
LLM Markdown Parser and Expense Analyzer

This module uses Large Language Models (LLM) to parse receipt data in Markdown format generated by VLM,
extract structured information, and perform preliminary expense categorization and anomaly detection.

Supported LLM models:
- Llama 3 (open source)
- Mistral (open source)
- GPT-4 (commercial)
- Claude 3 (commercial)
- Qwen 2 (open source)
"""

import json
import argparse
import logging
import re
import requests
from typing import Dict, Any, Optional, List, Tuple

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MarkdownExpenseAnalyzer:
    """Main class for parsing Markdown receipts and performing analysis"""
    
    def __init__(self, model_name: str = "llama3", api_key: Optional[str] = None, api_endpoint: Optional[str] = None):
        """
        Initialize the analyzer
        
        Args:
            model_name: Name of the LLM model to use
            api_key: API key (if using commercial models)
            api_endpoint: API endpoint URL (if using self-hosted models)
        """
        self.model_name = model_name.lower()
        self.api_key = api_key
        self.api_endpoint = api_endpoint
        self.supported_models = ["llama3", "mistral", "gpt-4", "claude-3", "qwen2-vl-2b"]
        
        if self.model_name not in self.supported_models:
            raise ValueError(f"Unsupported model: {model_name}. Supported models: {', '.join(self.supported_models)}")
            
        if self.model_name in ["gpt-4", "claude-3"] and not api_key:
            raise ValueError(f"Using {model_name} requires an API key")
            
        logger.info(f"Initializing MarkdownExpenseAnalyzer with model: {model_name}")

    def generate_parsing_prompt(self, markdown_text: str) -> str:
        """
        Generate a prompt for LLM to parse Markdown
        
        Args:
            markdown_text: Markdown receipt text generated by VLM
            
        Returns:
            A prompt suitable for the current model
        """
        base_prompt = f"""
Please parse the following receipt data in Markdown format and extract structured JSON information.
The JSON should include the following main fields:

- `merchant_info`: Contains `name`, `address`, `phone`, and other merchant information.
- `transaction_info`: Contains `date`, `time`, `receipt_number`, and other transaction details.
- `items`: A list containing information for each item, with `description`, `quantity`, `unit_price`, `total_price`.
- `price_summary`: Contains `subtotal`, `tax`, `total`, `discounts`, and other price summary information.
- `payment_info`: Contains `method`, `card_last_four`, `authorization_code`, and other payment details.
- `other_info`: Contains other information such as return policies, membership details, promotions, etc.

Please carefully parse tables and markings in the Markdown to ensure data accuracy.
If certain information doesn't exist, set the corresponding field to null or omit it.
Please output only the JSON result without any additional explanations or text.

Markdown data:
```markdown
{markdown_text}
```

Please output the JSON result:
"""
        
        # Fine-tune prompts for different models (if needed)
        if self.model_name == "gpt-4":
            return base_prompt + "\nPlease ensure the output is valid JSON format without any additional text or explanations."
        elif self.model_name == "claude-3":
            return base_prompt + "\nPlease output in JSON format without any additional explanations or Markdown formatting."
        
        return base_prompt

    def call_llm_api(self, prompt: str) -> str:
        """
        Call LLM API (open source or commercial)
        """
        logger.info(f"Calling LLM model: {self.model_name}")

        try:
            if self.model_name == "llama3":
                # Ollama API or other Llama 3 API
                url = self.api_endpoint or "http://localhost:11434/api/chat"
                headers = {"Content-Type": "application/json"}
                payload = {
                    "model": "llama3",
                    "messages": [
                        {"role": "system", "content": "You are a receipt structuring assistant"},
                        {"role": "user", "content": prompt}
                    ],
                    "stream": False
                }
                response = requests.post(url, headers=headers, json=payload, timeout=60)
                response.raise_for_status()
                result = response.json()
                return result["message"]["content"]
                
            elif self.model_name == "mistral":
                # Mistral API
                url = self.api_endpoint or "http://localhost:11434/api/generate"
                headers = {"Content-Type": "application/json"}
                payload = {
                    "model": "mistral",
                    "prompt": prompt,
                    "stream": False
                }
                response = requests.post(url, headers=headers, json=payload, timeout=60)
                response.raise_for_status()
                result = response.json()
                return result.get("response", "")
                
            elif self.model_name == "gpt-4":
                # OpenAI API
                url = self.api_endpoint or "https://api.openai.com/v1/chat/completions"
                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self.api_key}"
                }
                payload = {
                    "model": "gpt-4",
                    "messages": [
                        {"role": "system", "content": "You are a professional receipt parsing assistant that outputs only structured data in JSON format."},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.1
                }
                response = requests.post(url, headers=headers, json=payload, timeout=60)
                response.raise_for_status()
                result = response.json()
                return result["choices"][0]["message"]["content"]
                
            elif self.model_name == "claude-3":
                # Anthropic API
                url = self.api_endpoint or "https://api.anthropic.com/v1/messages"
                headers = {
                    "Content-Type": "application/json",
                    "x-api-key": self.api_key,
                    "anthropic-version": "2023-06-01"
                }
                payload = {
                    "model": "claude-3-sonnet-20240229",
                    "max_tokens": 4000,
                    "messages": [
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.1
                }
                response = requests.post(url, headers=headers, json=payload, timeout=60)
                response.raise_for_status()
                result = response.json()
                return result["content"][0]["text"]
                
            elif self.model_name == "qwen2-vl-2b":
                # Qwen API
                url = self.api_endpoint or "http://localhost:8000/v1/chat/completions"
                headers = {"Content-Type": "application/json"}
                payload = {
                    "model": "Qwen2-VL-2B",
                    "messages": [
                        {"role": "system", "content": "You are a receipt structuring assistant"},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.1
                }
                response = requests.post(url, headers=headers, json=payload, timeout=60)
                response.raise_for_status()
                result = response.json()
                return result["choices"][0]["message"]["content"]
                
            else:
                raise ValueError(f"Unimplemented model: {self.model_name}")
                
        except Exception as e:
            logger.error(f"LLM model call failed: {str(e)}")
            raise

    def parse_markdown(self, markdown_text: str) -> Dict[str, Any]:
        """
        Use LLM to parse Markdown text and return structured JSON
        
        Args:
            markdown_text: Receipt text in Markdown format
            
        Returns:
            Dictionary containing structured receipt information
        """
        logger.info("Starting Markdown receipt parsing")
        
        # 1. Generate prompt
        prompt = self.generate_parsing_prompt(markdown_text)
        
        # 2. Call LLM API
        json_string = self.call_llm_api(prompt)
        
        # 3. Parse JSON result
        try:
            # Clean possible Markdown code block markers
            if json_string.strip().startswith("```json"):
                json_string = json_string.strip()[7:-3].strip()
            elif json_string.strip().startswith("```"):
                 json_string = json_string.strip()[3:-3].strip()
                 
            structured_data = json.loads(json_string)
            logger.info("Markdown parsing successful")
            return structured_data
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON returned by LLM: {str(e)}")
            logger.error(f"Original output: {json_string}")
            # Could try using regex or other methods for remedial parsing
            # Or just raise an error
            raise ValueError("Unable to parse LLM's JSON output")
        except Exception as e:
            logger.error(f"Unknown error occurred while processing LLM output: {str(e)}")
            raise

    def categorize_expense(self, structured_data: Dict[str, Any]) -> str:
        """
        Categorize expenses based on structured data (initial implementation)
        
        Args:
            structured_data: Dictionary of parsed receipt data
            
        Returns:
            Expense category string (e.g., Food, Transportation, Office Supplies, etc.)
        """
        logger.info("Starting expense categorization")
        
        merchant_name = structured_data.get("merchant_info", {}).get("name", "").lower()
        items = structured_data.get("items", [])
        
        # Enhanced multilingual keyword classification rules
        food_keywords = ["咖啡", "餐廳", "餐飲", "麥當勞", "星巴克", "便利商店", 
                         "coffee", "restaurant", "food", "mcdonald", "starbucks", "cafe", 
                         "pizza", "burger", "sandwich", "grocery", "market", "bakery"]
        
        market_keywords = ["超市", "家樂福", "全聯", "超商", 
                          "supermarket", "grocery", "market", "walmart", "target", "costco", "store"]
        
        transport_keywords = ["加油站", "計程車", "捷運", "高鐵", "車票", 
                             "gas", "taxi", "uber", "lyft", "train", "bus", "metro", "ticket"]
        
        hotel_keywords = ["飯店", "旅館", "住宿", 
                         "hotel", "motel", "inn", "lodge", "accommodation", "stay"]
        
        office_keywords = ["辦公用品", "文具店", "文具", "筆", "紙", "訂書機", "辦公", 
                          "office", "stationery", "supplies", "pen", "paper", "stapler"]
        
        # Simple keyword-based classification rules
        if any(keyword in merchant_name for keyword in food_keywords):
            return "Food & Dining"
        
        if any(keyword in merchant_name for keyword in market_keywords):
            # Need to check item contents to determine if it's food or office supplies
            is_office_supply = False
            for item in items:
                desc = item.get("description", "").lower()
                if any(supply_keyword in desc for supply_keyword in office_keywords):
                    is_office_supply = True
                    break
            if is_office_supply:
                return "Office Supplies"
            else:
                return "Groceries"
                
        if any(keyword in merchant_name for keyword in transport_keywords):
            return "Transportation"
            
        if any(keyword in merchant_name for keyword in hotel_keywords):
            return "Travel & Accommodation"
            
        if any(keyword in merchant_name for keyword in office_keywords):
             return "Office Supplies"

        # If merchant name doesn't help, try to determine from item descriptions
        for item in items:
            desc = item.get("description", "").lower()
            if any(keyword in desc for keyword in transport_keywords):
                return "Transportation"
            if any(keyword in desc for keyword in hotel_keywords):
                return "Travel & Accommodation"
            if any(keyword in desc for keyword in office_keywords):
                return "Office Supplies"

        logger.warning("Unable to determine expense category, classifying as 'Other'")
        return "Other"

    def detect_anomalies(self, structured_data: Dict[str, Any], category: str) -> List[str]:
        """
        Detect expense anomalies (initial implementation)
        
        Args:
            structured_data: Dictionary of parsed receipt data
            category: Expense category
            
        Returns:
            List of detected anomaly descriptions
        """
        logger.info("Starting anomaly detection")
        anomalies = []
        
        total_amount_str = structured_data.get("price_summary", {}).get("total")
        total_amount = 0.0
        if total_amount_str:
            # Try to extract the numeric part
            match = re.search(r'[\d,.]+', str(total_amount_str))
            if match:
                try:
                    total_amount = float(match.group(0).replace(',', ''))
                except ValueError:
                    logger.warning(f"Unable to parse number from total amount string '{total_amount_str}'")
            else:
                 logger.warning(f"Unable to find number in total amount string '{total_amount_str}'")

        # 1. Amount anomaly detection (example rules)
        thresholds = {
            "Food & Dining": 200,
            "Transportation": 500,
            "Office Supplies": 300,
            "Groceries": 400,
            "Travel & Accommodation": 1000,
            "Other": 150
        }
        
        if category in thresholds and total_amount > thresholds[category]:
            anomalies.append(f"Amount anomaly: {category} expense ${total_amount:.2f} exceeds threshold ${thresholds[category]}")
            
        # 2. Time anomaly detection (example rule: office supplies purchased outside working hours)
        transaction_time_str = structured_data.get("transaction_info", {}).get("time")
        if category == "Office Supplies" and transaction_time_str:
            try:
                hour = int(transaction_time_str.split(':')[0])
                if hour < 8 or hour > 19: # Assuming working hours are 8:00 - 19:00
                    anomalies.append(f"Time anomaly: Office supplies purchased outside working hours ({transaction_time_str})")
            except (ValueError, IndexError):
                logger.warning(f"Unable to parse transaction time: {transaction_time_str}")

        # 3. Missing key information detection
        if not structured_data.get("merchant_info", {}).get("name"):
            anomalies.append("Missing information: Merchant name not found")
        if not structured_data.get("transaction_info", {}).get("date"):
            anomalies.append("Missing information: Transaction date not found")
        if not total_amount_str:
             anomalies.append("Missing information: Total amount not found")
             
        # 4. Check for discounts with zero or negative amounts (example)
        discount = structured_data.get("price_summary", {}).get("discounts")
        if discount is not None:
            try:
                # Extract discount number
                discount_val_str = re.search(r'[-+]?[\d,.]+', str(discount))
                if discount_val_str:
                    discount_val = float(discount_val_str.group(0).replace(',', ''))
                    if discount_val <= 0:
                         anomalies.append(f"Suspicious discount: Discount information exists but amount is {discount_val}")
                else:
                    # If there's a discount field but can't parse the value, may flag as anomaly
                    anomalies.append(f"Suspicious discount: Unable to parse discount amount '{discount}'")
            except ValueError:
                 anomalies.append(f"Suspicious discount: Unable to parse discount amount '{discount}'")

        if anomalies:
            logger.warning(f"Detected {len(anomalies)} anomalies")
        else:
            logger.info("No obvious anomalies detected")
            
        return anomalies

    def analyze(self, markdown_text: str) -> Dict[str, Any]:
        """
        Execute complete parsing and analysis workflow
        
        Args:
            markdown_text: Receipt text in Markdown format
            
        Returns:
            Dictionary containing structured data, categorization, and anomaly detection results
        """
        # 1. Parse Markdown
        structured_data = self.parse_markdown(markdown_text)
        
        # 2. Expense categorization
        category = self.categorize_expense(structured_data)
        
        # 3. Anomaly detection
        anomalies = self.detect_anomalies(structured_data, category)
        
        # 4. Combine results
        analysis_result = {
            "structured_data": structured_data,
            "analysis": {
                "category": category,
                "anomalies_detected": anomalies
            }
        }
        
        return analysis_result


def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Parse Markdown receipts and perform analysis')
    parser.add_argument('markdown_file', type=str, help='Path to Markdown file')
    parser.add_argument('--model', type=str, default='llama3', 
                        choices=['llama3', 'mistral', 'gpt-4', 'claude-3', 'qwen2-vl-2b'],
                        help='Name of LLM model to use')
    parser.add_argument('--api-key', type=str, help='API key (required for commercial models)')
    parser.add_argument('--api-endpoint', type=str, help='API endpoint URL (required for self-hosted models)')
    parser.add_argument('--output', type=str, help='Path to output JSON file')
    
    args = parser.parse_args()
    
    try:
        # Read Markdown file
        with open(args.markdown_file, 'r', encoding='utf-8') as f:
            markdown_text = f.read()
        
        # Initialize analyzer
        analyzer = MarkdownExpenseAnalyzer(
            model_name=args.model, 
            api_key=args.api_key,
            api_endpoint=args.api_endpoint
        )
        
        # Analyze Markdown
        analysis_result = analyzer.analyze(markdown_text)
        
        # Output results
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)
            print(f"Analysis results saved to: {args.output}")
        else:
            print(json.dumps(analysis_result, ensure_ascii=False, indent=2))
            
    except Exception as e:
        print(f"Error: {str(e)}")
        return 1
        
    return 0


if __name__ == "__main__":
    exit(main())
